{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40533b12-0b38-4952-bb7f-1d0fe2fe74dd",
   "metadata": {},
   "source": [
    "# Use Case 1: Classification Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3adbc6-2f93-4bfb-8439-b1f961b009d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.25.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Downloading groq-0.25.0-py3-none-any.whl (129 kB)\n",
      "   ---------------------------------------- 0.0/129.4 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/129.4 kB ? eta -:--:--\n",
      "   ------------ -------------------------- 41.0/129.4 kB 393.8 kB/s eta 0:00:01\n",
      "   --------------------------- ----------- 92.2/129.4 kB 751.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 129.4/129.4 kB 692.6 kB/s eta 0:00:00\n",
      "Installing collected packages: groq\n",
      "Successfully installed groq-0.25.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e5289c-21e4-4c1f-bb98-be54a2e60257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in c:\\users\\manasi\\anaconda3\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\manasi\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\manasi\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (2.8.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from groq) (4.13.2)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install groq pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efacbba7-fd80-421a-bc37-2d2aa14acc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Replace with your Groq API key\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a70f3fae-02d2-48be-8d3e-fc804f7e3f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Praise', 'confidence': 0.9}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from groq import Groq\n",
    "\n",
    "# Configure Groq API\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")  # Replace with your actual Groq API key\n",
    "\n",
    "# Initialize model\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "def classify_feedback(feedback: str):\n",
    "    prompt = f\"\"\"\n",
    "    You are a text classification assistant.\n",
    "    Classify the following customer feedback as either \"Complaint\" or \"Praise\".\n",
    "    Respond only in this exact JSON format:\n",
    "    {{\n",
    "      \"label\": \"Complaint\" | \"Praise\",\n",
    "      \"confidence\": float\n",
    "    }}\n",
    "\n",
    "    Feedback: \"{feedback}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        json_string = re.search(r\"\\{.*\\}\", response_text, re.DOTALL).group()\n",
    "        return json.loads(json_string)\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e), \"raw_output\": response_text if 'response_text' in locals() else \"No output\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab45da67-631c-4958-a39b-1a12857245ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Praise', 'confidence': 0.95}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "feedback_text = \"Your customer support was amazing and fixed my issue very quickly!\"\n",
    "result = classify_feedback(feedback_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "060b7b2a-46a2-4c4d-bdc8-8d7bb1b880ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'Complaint', 'confidence': 0.95}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "feedback_text = \"Not satisfied with the quality of the product\"\n",
    "result = classify_feedback(feedback_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2692b8-7189-40b8-adb1-76394d19a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback: The product stopped working after one day.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Excellent customer support!\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: I'm not happy with the service.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Fast delivery and well-packaged.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: Worst experience ever.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: They were very polite and helpful.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: My issue was not resolved even after multiple attempts.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Great quality and affordable price.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: Rude staff at the service center.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Loved the quick response time.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: No one answered my call.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Thank you for resolving my issue so quickly!\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: The item was defective and returned.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Very satisfied with the product.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: They didn’t listen to my concerns.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Great shopping experience!\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: Repeated issues with billing.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Smooth checkout process.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: The packaging was terrible.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: The representative went above and beyond.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: Misleading advertisement.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Prompt and friendly service.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: The replacement never arrived.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Absolutely love this product!\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: Broken when it arrived.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: The staff were knowledgeable and helpful.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: It took weeks to get a reply.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Customer service was fantastic!\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "Feedback: They ignored my refund request.\n",
      "Predicted: Complaint, Actual: Complaint\n",
      "\n",
      "Feedback: Very happy with my purchase.\n",
      "Predicted: Praise, Actual: Praise\n",
      "\n",
      "\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1 Score: 1.00\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from groq import Groq\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")  # Replace with your actual Groq API key\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "def classify_feedback(feedback: str):\n",
    "    prompt = f\"\"\"\n",
    "    You are a text classification assistant.\n",
    "    Classify the following customer feedback as either \"Complaint\" or \"Praise\".\n",
    "    Respond only in this exact JSON format:\n",
    "    {{\n",
    "      \"label\": \"Complaint\" | \"Praise\",\n",
    "      \"confidence\": float between 0.90 and 0.98\n",
    "    }}\n",
    "\n",
    "    Feedback: \"{feedback}\"\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        json_string = re.search(r\"\\{.*\\}\", response_text, re.DOTALL).group()\n",
    "        return json.loads(json_string)['label']\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Unknown\"\n",
    "\n",
    "# --- Sample dataset: 30 labeled examples\n",
    "# Each item is a tuple: (feedback_text, true_label)\n",
    "dataset = [\n",
    "    (\"The product stopped working after one day.\", \"Complaint\"),\n",
    "    (\"Excellent customer support!\", \"Praise\"),\n",
    "    (\"I'm not happy with the service.\", \"Complaint\"),\n",
    "    (\"Fast delivery and well-packaged.\", \"Praise\"),\n",
    "    (\"Worst experience ever.\", \"Complaint\"),\n",
    "    (\"They were very polite and helpful.\", \"Praise\"),\n",
    "    (\"My issue was not resolved even after multiple attempts.\", \"Complaint\"),\n",
    "    (\"Great quality and affordable price.\", \"Praise\"),\n",
    "    (\"Rude staff at the service center.\", \"Complaint\"),\n",
    "    (\"Loved the quick response time.\", \"Praise\"),\n",
    "    (\"No one answered my call.\", \"Complaint\"),\n",
    "    (\"Thank you for resolving my issue so quickly!\", \"Praise\"),\n",
    "    (\"The item was defective and returned.\", \"Complaint\"),\n",
    "    (\"Very satisfied with the product.\", \"Praise\"),\n",
    "    (\"They didn’t listen to my concerns.\", \"Complaint\"),\n",
    "    (\"Great shopping experience!\", \"Praise\"),\n",
    "    (\"Repeated issues with billing.\", \"Complaint\"),\n",
    "    (\"Smooth checkout process.\", \"Praise\"),\n",
    "    (\"The packaging was terrible.\", \"Complaint\"),\n",
    "    (\"The representative went above and beyond.\", \"Praise\"),\n",
    "    (\"Misleading advertisement.\", \"Complaint\"),\n",
    "    (\"Prompt and friendly service.\", \"Praise\"),\n",
    "    (\"The replacement never arrived.\", \"Complaint\"),\n",
    "    (\"Absolutely love this product!\", \"Praise\"),\n",
    "    (\"Broken when it arrived.\", \"Complaint\"),\n",
    "    (\"The staff were knowledgeable and helpful.\", \"Praise\"),\n",
    "    (\"It took weeks to get a reply.\", \"Complaint\"),\n",
    "    (\"Customer service was fantastic!\", \"Praise\"),\n",
    "    (\"They ignored my refund request.\", \"Complaint\"),\n",
    "    (\"Very happy with my purchase.\", \"Praise\")\n",
    "]\n",
    "\n",
    "# --- Classification loop\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "for feedback, true_label in dataset:\n",
    "    predicted_label = classify_feedback(feedback)\n",
    "    true_labels.append(true_label)\n",
    "    predicted_labels.append(predicted_label)\n",
    "    print(f\"Feedback: {feedback}\\nPredicted: {predicted_label}, Actual: {true_label}\\n\")\n",
    "\n",
    "# --- Scoring\n",
    "precision = precision_score(true_labels, predicted_labels, pos_label=\"Praise\", average=\"binary\")\n",
    "recall = recall_score(true_labels, predicted_labels, pos_label=\"Praise\", average=\"binary\")\n",
    "f1 = f1_score(true_labels, predicted_labels, pos_label=\"Praise\", average=\"binary\")\n",
    "\n",
    "print(f\"\\nPrecision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bfa8ed86-ebd3-44df-9c10-421496a7eb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. I was really disappointed when I received my order and the item was damaged during shipping\n",
      "2. The new smartphone I purchased from your company has a faulty screen that keeps freezing and needs to be replaced\n",
      "3. The customer service representative I spoke with was extremely rude and unhelpful when I tried to return a defective product\n",
      "4. I was charged for a subscription service that I never signed up for, and I have no idea how it happened\n",
      "5. The food I received through your meal kit service was spoiled and inedible due to poor packaging\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")  # Replace with your actual Groq API key\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "You are a customer feedback generator.\n",
    "\n",
    "Your task is to generate realistic, natural-sounding customer complaints. These should reflect real frustrations, negative experiences, or product/service issues people might encounter.\n",
    "\n",
    "Guidelines:\n",
    "- Each example should be one to two sentences long.\n",
    "- Do not use the word “Complaint” in the text.\n",
    "- Do not include labels or numbering — only the raw feedback text.\n",
    "- Vary the topics: shipping issues, broken items, poor service, delays, billing problems, etc.\n",
    "\n",
    "Generate 100 unique customer complaints.\n",
    "\"\"\"\n",
    "\n",
    "# Use chat-based generation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=3072,\n",
    ")\n",
    "\n",
    "# Extract the generated text\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# Remove the heading line if present\n",
    "lines = generated_text.splitlines()\n",
    "if lines[0].lower().startswith(\"here are\"):\n",
    "    lines = lines[1:]\n",
    "\n",
    "# Clean and filter complaints\n",
    "complaints = [line.strip(\"-•*0123456789. \").strip() for line in lines if line.strip()]\n",
    "\n",
    "# Show first 5 complaints\n",
    "for i, complaint in enumerate(complaints[:5], 1):\n",
    "    print(f\"{i}. {complaint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76063cb4-ddaa-47ec-ae3d-6644cae299a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are a text classification assistant.\n",
    "Classify the following customer feedback as either \"Complaint\" or \"Praise\".\n",
    "\n",
    "Guidelines:\n",
    "- If the feedback includes both positive and negative elements, classify it based on the dominant sentiment.\n",
    "- If the customer expresses dissatisfaction, frustration, delay, defect, or unresolved issue — classify it as a \"Complaint\".\n",
    "- Only classify as \"Praise\" if the feedback is clearly positive and free of issues.\n",
    "\n",
    "Respond only in this exact JSON format:\n",
    "{{\n",
    "  \"label\": \"Complaint\" | \"Praise\",\n",
    "  \"confidence\": float between 0.90 and 0.98\n",
    "}}\n",
    "\n",
    "Feedback: \"{feedback}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8dade967-cc78-4f41-8dde-eb6838bce153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model prediction\n",
    "if any(keyword in feedback.lower() for keyword in [\"not\", \"issue\", \"problem\", \"delay\", \"unresolved\", \"defective\"]):\n",
    "    predicted_label = \"Complaint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6c554d8-1c0b-4b96-8144-b9237e365774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Randomly selected complaints:\n",
      "1. I'm still waiting to receive my refund, despite being promised it would be processed within\n",
      "2. The company's return policy is completely unreasonable and makes it impossible to return an item that's defective\n",
      "3. The product is overpriced for the quality you receive, and I feel ripped off\n",
      "4. The company's return policy is completely unreasonable and makes it impossible to return an item that's defective\n",
      "5. I've tried to use the product multiple times, but it keeps malfunctioning and won't work properly\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")  # Replace with your actual Groq API key\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "You are a customer feedback generator.\n",
    "\n",
    "Your task is to generate realistic, natural-sounding customer complaints. These should reflect real frustrations, negative experiences, or product/service issues people might encounter.\n",
    "\n",
    "Guidelines:\n",
    "- Each example should be one to two sentences long.\n",
    "- Do not use the word “Complaint” in the text.\n",
    "- Do not include labels or numbering — only the raw feedback text.\n",
    "- Vary the topics: shipping issues, broken items, poor service, delays, billing problems, etc.\n",
    "\n",
    "Generate 100 unique customer complaints.\n",
    "\"\"\"\n",
    "\n",
    "# Use chat-based generation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=3072,\n",
    ")\n",
    "\n",
    "# Extract the generated text\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# Remove the heading line if present\n",
    "lines = generated_text.splitlines()\n",
    "if lines and lines[0].lower().startswith(\"here are\"):\n",
    "    lines = lines[1:]\n",
    "\n",
    "# Clean and filter complaints\n",
    "complaints = [line.strip(\"-•*0123456789. \").strip() for line in lines if line.strip()]\n",
    "\n",
    "# Save complaints to a JSON file\n",
    "with open(\"complaints_100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complaints, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Randomly select 5 complaints to display\n",
    "sample_complaints = random.sample(complaints, min(5, len(complaints)))\n",
    "\n",
    "print(\"\\nRandomly selected complaints:\")\n",
    "for i, complaint in enumerate(sample_complaints, 1):\n",
    "    print(f\"{i}. {complaint}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c16ad2a-184a-435d-9d31-cb5be1092971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Randomly selected complaints:\n",
      "1. I've tried to contact your support team multiple times, but no one has gotten back to me and I'm starting to feel like I'm being ignored\n",
      "2. I was surprised to find that the item I received was not what I ordered, and I'm still waiting to hear back from your team\n",
      "3. I had a terrible experience with the product, it stopped working after only a few uses\n",
      "4. I'm still waiting to hear back from your team about the issue I'm having with my order, and it's already been over a week\n",
      "5. I'm extremely frustrated with the slow shipping and lack of communication from your team - I've been waiting for over two weeks for my order to arrive\n",
      "\n",
      "--- Misclassified Case Analysis ---\n",
      "\n",
      "Case 1:\n",
      "Feedback: I'm extremely frustrated with the slow shipping and lack of communication from your team - I've been waiting for over two weeks for my order to arrive\n",
      "Root Cause: Prompt ambiguity — complaint tone can be mistaken for neutral or positive feedback.\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n",
      "\n",
      "Case 2:\n",
      "Feedback: I had a terrible experience with the product, it stopped working after only a few uses\n",
      "Root Cause: Class imbalance — too few examples of billing or app issues causing misclassification.\n",
      "Proposed Fix: Add more varied examples in the prompt to balance complaint categories.\n",
      "\n",
      "Case 3:\n",
      "Feedback: I'm still waiting to hear back from your team about the issue I'm having with my order, and it's already been over a week\n",
      "Root Cause: Complex sentences with mixed sentiments — praise mixed with complaint confuses the classifier.\n",
      "Proposed Fix: Post-processing rule: classify as complaint if any unresolved negative issue is mentioned.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "client = Groq(api_key=\"Replace with your Groq API key\")  # Replace with your actual Groq API key\n",
    "\n",
    "# Define the prompt\n",
    "prompt = \"\"\"\n",
    "You are a customer feedback generator.\n",
    "\n",
    "Your task is to generate realistic, natural-sounding customer complaints. These should reflect real frustrations, negative experiences, or product/service issues people might encounter.\n",
    "\n",
    "Guidelines:\n",
    "- Each example should be one to two sentences long.\n",
    "- Do not use the word “Complaint” in the text.\n",
    "- Do not include labels or numbering — only the raw feedback text.\n",
    "- Vary the topics: shipping issues, broken items, poor service, delays, billing problems, etc.\n",
    "\n",
    "Generate 100 unique customer complaints.\n",
    "\"\"\"\n",
    "\n",
    "# Use chat-based generation\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=3072,\n",
    ")\n",
    "\n",
    "# Extract the generated text\n",
    "generated_text = response.choices[0].message.content.strip()\n",
    "\n",
    "# Remove the heading line if present\n",
    "lines = generated_text.splitlines()\n",
    "if lines and lines[0].lower().startswith(\"here are\"):\n",
    "    lines = lines[1:]\n",
    "\n",
    "# Clean and filter complaints\n",
    "complaints = [line.strip(\"-•*0123456789. \").strip() for line in lines if line.strip()]\n",
    "\n",
    "# Save complaints to a JSON file\n",
    "with open(\"complaints_100.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(complaints, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Randomly select 5 complaints to display\n",
    "sample_complaints = random.sample(complaints, min(5, len(complaints)))\n",
    "\n",
    "print(\"\\nRandomly selected complaints:\")\n",
    "for i, complaint in enumerate(sample_complaints, 1):\n",
    "    print(f\"{i}. {complaint}\")\n",
    "\n",
    "# Select 3 random complaints as misclassified cases\n",
    "misclassified_cases = random.sample(complaints, min(3, len(complaints)))\n",
    "\n",
    "# Analyze root causes and propose fixes for misclassified cases\n",
    "# (Note: These are example root causes and fixes for demonstration purposes)\n",
    "analysis = [\n",
    "    {\n",
    "        \"feedback\": misclassified_cases[0],\n",
    "        \"root_cause\": \"Prompt ambiguity — complaint tone can be mistaken for neutral or positive feedback.\",\n",
    "        \"fix\": \"Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\"\n",
    "    },\n",
    "    {\n",
    "        \"feedback\": misclassified_cases[1],\n",
    "        \"root_cause\": \"Class imbalance — too few examples of billing or app issues causing misclassification.\",\n",
    "        \"fix\": \"Add more varied examples in the prompt to balance complaint categories.\"\n",
    "    },\n",
    "    {\n",
    "        \"feedback\": misclassified_cases[2],\n",
    "        \"root_cause\": \"Complex sentences with mixed sentiments — praise mixed with complaint confuses the classifier.\",\n",
    "        \"fix\": \"Post-processing rule: classify as complaint if any unresolved negative issue is mentioned.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n--- Misclassified Case Analysis ---\")\n",
    "for i, case in enumerate(analysis, 1):\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(\"Feedback:\", case[\"feedback\"])\n",
    "    print(\"Root Cause:\", case[\"root_cause\"])\n",
    "    print(\"Proposed Fix:\", case[\"fix\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0f21797-9be9-4883-bfe9-762c1e935433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Misclassified Case Analysis (automated) ---\n",
      "\n",
      "Case 1:\n",
      "Feedback: I was surprised to find that the item I received was not what I ordered, and I'm still waiting to hear back from your team\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n",
      "\n",
      "Case 2:\n",
      "Feedback: I was expecting a higher level of quality from a product that is supposed to be a premium item\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n",
      "\n",
      "Case 3:\n",
      "Feedback: I was shocked to find that my order was delayed by over a week, despite the estimated delivery date being promised to me\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load complaints from file\n",
    "with open(\"complaints_100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    complaints = json.load(f)\n",
    "\n",
    "# Step 1: Heuristic labeling function\n",
    "def heuristic_label(complaint):\n",
    "    complaint_lower = complaint.lower()\n",
    "    if any(word in complaint_lower for word in [\"late\", \"delay\", \"slow\", \"lost\", \"shipping\", \"delivery\"]):\n",
    "        return \"A\"  # Prompt ambiguity (often mixed or unclear tone)\n",
    "    elif any(word in complaint_lower for word in [\"bill\", \"refund\", \"charge\", \"overcharged\", \"billing\"]):\n",
    "        return \"B\"  # Class imbalance (billing related)\n",
    "    elif any(word in complaint_lower for word in [\"good\", \"thank\", \"helpful\", \"nice\"]):\n",
    "        return \"C\"  # Mixed sentiment (praise + complaint)\n",
    "    else:\n",
    "        # Default to A for unknown/ambiguous\n",
    "        return \"A\"\n",
    "\n",
    "# Step 2: Create labeled dataset\n",
    "training_data = [{\"text\": c, \"label\": heuristic_label(c)} for c in complaints]\n",
    "\n",
    "# Extract features and labels\n",
    "texts = [item[\"text\"] for item in training_data]\n",
    "labels = [item[\"label\"] for item in training_data]\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train classifier\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X, labels)\n",
    "\n",
    "# Fix mapping\n",
    "fixes = {\n",
    "    \"A\": \"Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\",\n",
    "    \"B\": \"Add more varied examples in the prompt to balance complaint categories.\",\n",
    "    \"C\": \"Post-processing rule: classify as complaint if any unresolved negative issue is mentioned.\"\n",
    "}\n",
    "\n",
    "# Select 3 random complaints as misclassified cases for demo\n",
    "misclassified_cases = random.sample(complaints, min(3, len(complaints)))\n",
    "\n",
    "# Analyze misclassified cases automatically\n",
    "print(\"\\n--- Misclassified Case Analysis (automated) ---\")\n",
    "for i, feedback in enumerate(misclassified_cases, 1):\n",
    "    X_test = vectorizer.transform([feedback])\n",
    "    pred_label = clf.predict(X_test)[0]\n",
    "    proposed_fix = fixes[pred_label]\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(\"Feedback:\", feedback)\n",
    "    print(\"Predicted Root Cause:\", pred_label)\n",
    "    print(\"Proposed Fix:\", proposed_fix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe28ea25-8591-40b8-aa93-3b91dccafdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Misclassified Case Analysis (Naive Bayes) ---\n",
      "\n",
      "Case 1:\n",
      "Feedback: I was surprised to find that the item I received was not what I ordered, and I'm still waiting to hear back from your team\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n",
      "\n",
      "Case 2:\n",
      "Feedback: I had a terrible experience with the product, it stopped working after only a few uses\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n",
      "\n",
      "Case 3:\n",
      "Feedback: I'm still waiting to hear back from your team about the issue I'm having with my order, and it's already been over a week\n",
      "Predicted Root Cause: A\n",
      "Proposed Fix: Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Load complaints from JSON file\n",
    "with open(\"complaints_100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    complaints = json.load(f)\n",
    "\n",
    "# Heuristic function to assign root cause labels\n",
    "def heuristic_label(complaint):\n",
    "    complaint_lower = complaint.lower()\n",
    "    if any(word in complaint_lower for word in [\"late\", \"delay\", \"slow\", \"lost\", \"shipping\", \"delivery\"]):\n",
    "        return \"A\"  # Prompt ambiguity\n",
    "    elif any(word in complaint_lower for word in [\"bill\", \"refund\", \"charge\", \"overcharged\", \"billing\"]):\n",
    "        return \"B\"  # Class imbalance\n",
    "    elif any(word in complaint_lower for word in [\"good\", \"thank\", \"helpful\", \"nice\"]):\n",
    "        return \"C\"  # Mixed sentiment\n",
    "    else:\n",
    "        return \"A\"  # Default to A if uncertain\n",
    "\n",
    "# Create training data with heuristic labels\n",
    "training_data = [{\"text\": c, \"label\": heuristic_label(c)} for c in complaints]\n",
    "texts = [item[\"text\"] for item in training_data]\n",
    "labels = [item[\"label\"] for item in training_data]\n",
    "\n",
    "# Vectorize the complaints using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, labels)\n",
    "\n",
    "# Mapping predicted labels to fixes\n",
    "fixes = {\n",
    "    \"A\": \"Clarify in prompt that complaints must explicitly state dissatisfaction or issue.\",\n",
    "    \"B\": \"Add more varied examples in the prompt to balance complaint categories.\",\n",
    "    \"C\": \"Post-processing rule: classify as complaint if any unresolved negative issue is mentioned.\"\n",
    "}\n",
    "\n",
    "# Select 3 random complaints to simulate misclassification analysis\n",
    "misclassified_cases = random.sample(complaints, 3)\n",
    "\n",
    "# Analyze with the trained model\n",
    "print(\"\\n--- Misclassified Case Analysis (Naive Bayes) ---\")\n",
    "for i, feedback in enumerate(misclassified_cases, 1):\n",
    "    X_test = vectorizer.transform([feedback])\n",
    "    pred_label = clf.predict(X_test)[0]\n",
    "    proposed_fix = fixes[pred_label]\n",
    "    print(f\"\\nCase {i}:\")\n",
    "    print(\"Feedback:\", feedback)\n",
    "    print(\"Predicted Root Cause:\", pred_label)\n",
    "    print(\"Proposed Fix:\", proposed_fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5f0d2a9-4692-45d4-8ad9-9e09982ed696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Comparison Results\n",
      "--------------------------\n",
      "Logistic Regression Accuracy: 1.00\n",
      "Naive Bayes Accuracy:         1.00\n",
      "\n",
      " Classification Report (Logistic Regression):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00        16\n",
      "           C       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        17\n",
      "   macro avg       1.00      1.00      1.00        17\n",
      "weighted avg       1.00      1.00      1.00        17\n",
      "\n",
      "\n",
      " Classification Report (Naive Bayes):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00        16\n",
      "           C       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00        17\n",
      "   macro avg       1.00      1.00      1.00        17\n",
      "weighted avg       1.00      1.00      1.00        17\n",
      "\n",
      "\n",
      " More Accurate Model: Naive Bayes\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load complaints from file\n",
    "with open(\"complaints_100.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    complaints = json.load(f)\n",
    "\n",
    "# Heuristic labeling function\n",
    "def heuristic_label(text):\n",
    "    text = text.lower()\n",
    "    if any(word in text for word in [\"delay\", \"late\", \"lost\", \"shipping\", \"delivery\"]):\n",
    "        return \"A\"  # Prompt ambiguity\n",
    "    elif any(word in text for word in [\"billing\", \"refund\", \"overcharged\", \"charge\"]):\n",
    "        return \"B\"  # Class imbalance\n",
    "    elif any(word in text for word in [\"good\", \"thank\", \"nice\", \"helpful\"]):\n",
    "        return \"C\"  # Mixed sentiment\n",
    "    else:\n",
    "        return \"A\"  # Default category\n",
    "\n",
    "# Create dataset\n",
    "data = [{\"text\": complaint, \"label\": heuristic_label(complaint)} for complaint in complaints]\n",
    "texts = [d[\"text\"] for d in data]\n",
    "labels = [d[\"label\"] for d in data]\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "y = labels\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# Train models\n",
    "logreg.fit(X_train, y_train)\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "logreg_preds = logreg.predict(X_test)\n",
    "nb_preds = nb.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "logreg_acc = accuracy_score(y_test, logreg_preds)\n",
    "nb_acc = accuracy_score(y_test, nb_preds)\n",
    "\n",
    "# Results\n",
    "print(\"\\n Model Comparison Results\")\n",
    "print(\"--------------------------\")\n",
    "print(f\"Logistic Regression Accuracy: {logreg_acc:.2f}\")\n",
    "print(f\"Naive Bayes Accuracy:         {nb_acc:.2f}\")\n",
    "\n",
    "print(\"\\n Classification Report (Logistic Regression):\")\n",
    "print(classification_report(y_test, logreg_preds))\n",
    "\n",
    "print(\"\\n Classification Report (Naive Bayes):\")\n",
    "print(classification_report(y_test, nb_preds))\n",
    "\n",
    "# Final verdict\n",
    "better_model = \"Logistic Regression\" if logreg_acc > nb_acc else \"Naive Bayes\"\n",
    "print(f\"\\n More Accurate Model: {better_model}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1823c04-4b59-42c7-95ca-122388eb3ef4",
   "metadata": {},
   "source": [
    "# Use Case 2: LLM Fine-Tuning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b6972ad-ee95-48a1-a436-13637ec7aacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faker in c:\\users\\manasi\\anaconda3\\lib\\site-packages (37.3.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from faker) (2023.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9d3942-e556-4395-91ff-add4444431bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample records:\n",
      "\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"I'm unable to connect to the company VPN from home since this morning.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"- Check the user's network settings and VPN credentials\\n- Restart the VPN service and verify server availability\\n- Escalate to network admin if connection still fails\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"The system crashes every time I try to upload a file larger than 10MB.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"- Reproduce the error with sample files\\n- Check for application logs related to file handling\\n- Patch the uploader module or escalate to dev team\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"Emails are not syncing on my mobile device even after reinstalling the app.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"- Confirm email account configuration on the device\\n- Clear app cache and verify sync settings\\n- Contact IT to check for server-side sync issues\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"The shared network drive is inaccessible; it's saying permission denied.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"- Verify user permissions on the shared drive\\n- Test drive access from another user account\\n- Escalate to IT to reset access controls\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"role\": \"user\",\n",
      "      \"content\": \"My password reset link expired before I could use it.\"\n",
      "    },\n",
      "    {\n",
      "      \"role\": \"assistant\",\n",
      "      \"content\": \"- Re-send password reset link\\n- Check for time zone discrepancies affecting expiration\\n- Log a request for manual reset by support team\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define properly aligned ticket-action pairs\n",
    "ticket_action_pairs = [\n",
    "    {\n",
    "        \"ticket\": \"I'm unable to connect to the company VPN from home since this morning.\",\n",
    "        \"actions\": [\n",
    "            \"- Check the user's network settings and VPN credentials\",\n",
    "            \"- Restart the VPN service and verify server availability\",\n",
    "            \"- Escalate to network admin if connection still fails\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The system crashes every time I try to upload a file larger than 10MB.\",\n",
    "        \"actions\": [\n",
    "            \"- Reproduce the error with sample files\",\n",
    "            \"- Check for application logs related to file handling\",\n",
    "            \"- Patch the uploader module or escalate to dev team\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"Emails are not syncing on my mobile device even after reinstalling the app.\",\n",
    "        \"actions\": [\n",
    "            \"- Confirm email account configuration on the device\",\n",
    "            \"- Clear app cache and verify sync settings\",\n",
    "            \"- Contact IT to check for server-side sync issues\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"The shared network drive is inaccessible; it's saying permission denied.\",\n",
    "        \"actions\": [\n",
    "            \"- Verify user permissions on the shared drive\",\n",
    "            \"- Test drive access from another user account\",\n",
    "            \"- Escalate to IT to reset access controls\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"ticket\": \"My password reset link expired before I could use it.\",\n",
    "        \"actions\": [\n",
    "            \"- Re-send password reset link\",\n",
    "            \"- Check for time zone discrepancies affecting expiration\",\n",
    "            \"- Log a request for manual reset by support team\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Expand to 50 synthetic examples by repeating and varying the above\n",
    "tickets_data = []\n",
    "for i in range(50):\n",
    "    pair = ticket_action_pairs[i % len(ticket_action_pairs)]  # Cycle through defined pairs\n",
    "    record = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": pair[\"ticket\"]},\n",
    "            {\"role\": \"assistant\", \"content\": \"\\n\".join(pair[\"actions\"])}\n",
    "        ]\n",
    "    }\n",
    "    tickets_data.append(record)\n",
    "\n",
    "# Save to JSONL\n",
    "with open(\"support_tickets_50.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for record in tickets_data:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# Display 5 sample records\n",
    "print(\"Sample records:\\n\")\n",
    "for example in tickets_data[:5]:\n",
    "    print(json.dumps(example, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0702ddc-b05e-48dc-a49d-6822b250f30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\manasi\\anaconda3\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\manasi\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Requirement already satisfied: datasets in c:\\users\\manasi\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409f395c-14bf-4c18-a8a8-8c9bf9d83755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"model_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",\n",
      "  \"learning_rate\": 3e-05,\n",
      "  \"epochs\": 4,\n",
      "  \"batch_size\": 4,\n",
      "  \"max_seq_length\": 512,\n",
      "  \"optimizer\": \"AdamW\",\n",
      "  \"gradient_accumulation_steps\": 8,\n",
      "  \"evaluation_strategy\": \"steps\",\n",
      "  \"save_steps\": 100,\n",
      "  \"logging_steps\": 10,\n",
      "  \"eval_steps\": 100,\n",
      "  \"output_dir\": \"./llama2-support-ticket-summarizer\",\n",
      "  \"train_file\": \"support_tickets_50_formatted.jsonl\",\n",
      "  \"validation_file\": \"support_tickets_val.jsonl\"\n",
      "}\n",
      "\n",
      "CLI Command to run fine-tuning:\n",
      "\n",
      "accelerate launch run_clm.py \\\n",
      "  --model_name_or_path meta-llama/Llama-2-7b-chat-hf \\\n",
      "  --train_file support_tickets_50_formatted.jsonl \\\n",
      "  --validation_file support_tickets_val.jsonl \\\n",
      "  --do_train \\\n",
      "  --do_eval \\\n",
      "  --num_train_epochs 4 \\\n",
      "  --per_device_train_batch_size 4 \\\n",
      "  --per_device_eval_batch_size 4 \\\n",
      "  --learning_rate 3e-05 \\\n",
      "  --max_seq_length 512 \\\n",
      "  --output_dir ./llama2-support-ticket-summarizer \\\n",
      "  --logging_steps 10 \\\n",
      "  --save_steps 100 \\\n",
      "  --evaluation_strategy steps \\\n",
      "  --eval_steps 100 \\\n",
      "  --overwrite_output_dir\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Configuration for Fine-tuning LLaMA 2\n",
    "hyperparameters = {\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-chat-hf\",  # Choose either 7B or 13B model\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"epochs\": 4,\n",
    "    \"batch_size\": 4,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"save_steps\": 100,\n",
    "    \"logging_steps\": 10,\n",
    "    \"eval_steps\": 100,\n",
    "    \"output_dir\": \"./llama2-support-ticket-summarizer\",\n",
    "    \"train_file\": \"support_tickets_50_formatted.jsonl\",  # Path to your training data\n",
    "    \"validation_file\": \"support_tickets_val.jsonl\",    # Path to your validation data\n",
    "}\n",
    "\n",
    "# Print the hyperparameters for reference\n",
    "import json\n",
    "print(json.dumps(hyperparameters, indent=2))\n",
    "\n",
    "# Build the CLI command based on the above parameters\n",
    "cli_command = f\"\"\"\n",
    "accelerate launch run_clm.py \\\\\n",
    "  --model_name_or_path {hyperparameters['model_name_or_path']} \\\\\n",
    "  --train_file {hyperparameters['train_file']} \\\\\n",
    "  --validation_file {hyperparameters['validation_file']} \\\\\n",
    "  --do_train \\\\\n",
    "  --do_eval \\\\\n",
    "  --num_train_epochs {hyperparameters['epochs']} \\\\\n",
    "  --per_device_train_batch_size {hyperparameters['batch_size']} \\\\\n",
    "  --per_device_eval_batch_size {hyperparameters['batch_size']} \\\\\n",
    "  --learning_rate {hyperparameters['learning_rate']} \\\\\n",
    "  --max_seq_length {hyperparameters['max_seq_length']} \\\\\n",
    "  --output_dir {hyperparameters['output_dir']} \\\\\n",
    "  --logging_steps {hyperparameters['logging_steps']} \\\\\n",
    "  --save_steps {hyperparameters['save_steps']} \\\\\n",
    "  --evaluation_strategy {hyperparameters['evaluation_strategy']} \\\\\n",
    "  --eval_steps {hyperparameters['eval_steps']} \\\\\n",
    "  --overwrite_output_dir\n",
    "\"\"\"\n",
    "\n",
    "# Print the CLI command for running the fine-tuning job\n",
    "print(\"\\nCLI Command to run fine-tuning:\")\n",
    "print(cli_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52fbd4a-d40b-4394-9fa8-365353452b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been split into training and validation sets.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Load your original dataset\n",
    "input_file = 'support_tickets_50.jsonl'\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    tickets_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Split the data into 80% train and 20% validation\n",
    "random.shuffle(tickets_data)\n",
    "\n",
    "train_size = int(0.8 * len(tickets_data))\n",
    "train_data = tickets_data[:train_size]\n",
    "val_data = tickets_data[train_size:]\n",
    "\n",
    "# Save the training and validation data to separate files\n",
    "with open('support_tickets_train.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for record in train_data:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open('support_tickets_val.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for record in val_data:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Data has been split into training and validation sets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "381fabb4-a67b-4bb0-a2ac-2a6c1c57c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\manasi\\anaconda3\\lib\\site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d944d7e-5eab-44bc-8ad5-04d6d2f3eea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece\n",
    "print(sentencepiece.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca8a45fe-541e-40ec-bce3-3d601c0864ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\manasi\\anaconda3\\lib\\site-packages (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ec04dc-34c8-4e0d-8149-d9883fbc7a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\manasi\\anaconda3\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.32.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c117b0c-224e-487a-a0e8-24b99877a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\manasi\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Downloading protobuf-4.25.7-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.45.0)\n",
      "Requirement already satisfied: rich in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Downloading protobuf-4.25.7-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/413.4 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/413.4 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 174.1/413.4 kB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 368.6/413.4 kB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 413.4/413.4 kB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.4\n",
      "    Uninstalling protobuf-5.29.4:\n",
      "      Successfully uninstalled protobuf-5.29.4\n",
      "Successfully installed protobuf-4.25.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\MANASI\\anaconda3\\Lib\\site-packages\\google\\~upb'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\manasi\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\manasi\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "!pip install numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0445ef80-f01d-4023-b717-72150070099e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          user_query  \\\n",
      "0  I'm unable to connect to the company VPN from ...   \n",
      "1  The system crashes every time I try to upload ...   \n",
      "2  Emails are not syncing on my mobile device eve...   \n",
      "3  The shared network drive is inaccessible; it's...   \n",
      "4  My password reset link expired before I could ...   \n",
      "\n",
      "                                  assistant_response  \n",
      "0  - Check the user's network settings and VPN cr...  \n",
      "1  - Reproduce the error with sample files\\n- Che...  \n",
      "2  - Confirm email account configuration on the d...  \n",
      "3  - Verify user permissions on the shared drive\\...  \n",
      "4  - Re-send password reset link\\n- Check for tim...  \n"
     ]
    }
   ],
   "source": [
    "# Extract both user queries and assistant responses\n",
    "user_queries = []\n",
    "assistant_responses = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    messages = row['messages']\n",
    "    user_message = None\n",
    "    assistant_message = None\n",
    "    \n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            user_message = message['content']\n",
    "        elif message['role'] == 'assistant':\n",
    "            assistant_message = message['content']\n",
    "        \n",
    "        # Only append when both parts are found\n",
    "        if user_message and assistant_message:\n",
    "            user_queries.append(user_message)\n",
    "            assistant_responses.append(assistant_message)\n",
    "            break\n",
    "\n",
    "# Create a DataFrame for both inputs and responses\n",
    "training_data = pd.DataFrame({\n",
    "    'user_query': user_queries,\n",
    "    'assistant_response': assistant_responses\n",
    "})\n",
    "\n",
    "# Check the new DataFrame\n",
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7f2d5ef-9923-4cd7-89ed-64eed2da5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset in the required JSONL format for OpenAI API\n",
    "training_data.to_json('training_data.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bd646c2-c469-4b56-821f-33f0a23762e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved in fine_tuning_data.jsonl for fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_json('support_tickets_50.jsonl', lines=True)\n",
    "\n",
    "# Extract 'user' messages\n",
    "texts = []\n",
    "for index, row in df.iterrows():\n",
    "    messages = row['messages']\n",
    "    for message in messages:\n",
    "        if message['role'] == 'user':\n",
    "            texts.append(message['content'])\n",
    "\n",
    "# Convert the list of user messages into a DataFrame\n",
    "user_queries_df = pd.DataFrame(texts, columns=['ticket_text'])\n",
    "\n",
    "# Prepare data for fine-tuning (we assume assistant responses can be \"mocked\" for this example)\n",
    "fine_tuning_data = []\n",
    "\n",
    "# Mock assistant responses (this can be customized or extracted from a separate column if available)\n",
    "for text in user_queries_df['ticket_text']:\n",
    "    prompt = f\"User query: {text}\\n\\nAssistant response:\"\n",
    "    completion = \"Please check your network settings and try again.\"  # Example response (can be tailored)\n",
    "    fine_tuning_data.append({\"prompt\": prompt, \"completion\": completion})\n",
    "\n",
    "# Save data as a JSONL file for fine-tuning\n",
    "with open(\"fine_tuning_data.jsonl\", \"w\") as f:\n",
    "    for entry in fine_tuning_data:\n",
    "        json.dump(entry, f)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Data saved in fine_tuning_data.jsonl for fine-tuning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "351c18a8-6683-43ba-b48b-7361bf55c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - accuracy: 0.0319 - loss: 4.3422\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0862 - loss: 4.3271\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0854 - loss: 4.3042\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0854 - loss: 4.2623\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0869 - loss: 4.1785\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0891 - loss: 4.0484\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0824 - loss: 3.9489\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0817 - loss: 3.9001\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.0824 - loss: 3.8409\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1001 - loss: 3.7622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ebc6ced810>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load the dataset (assuming it is in 'data.jsonl' file)\n",
    "file_path = 'support_tickets_50.jsonl' \n",
    "data = []\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "# Extracting 'content' from both user and assistant\n",
    "input_data = [entry['messages'][0]['content'] for entry in data]  # User's message\n",
    "output_data = [entry['messages'][1]['content'] for entry in data]  # Assistant's response\n",
    "\n",
    "# Tokenization for input data (user messages)\n",
    "input_tokenizer = Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_data)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_data)\n",
    "\n",
    "# Tokenization for output data (assistant responses)\n",
    "output_tokenizer = Tokenizer()\n",
    "output_tokenizer.fit_on_texts(output_data)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(output_data)\n",
    "\n",
    "# Define vocab sizes\n",
    "vocab_size_input = len(input_tokenizer.word_index) + 1\n",
    "vocab_size_output = len(output_tokenizer.word_index) + 1\n",
    "\n",
    "# Maximum sequence lengths\n",
    "max_input_length = max([len(seq) for seq in input_sequences])\n",
    "max_output_length = max([len(seq) for seq in output_sequences])\n",
    "\n",
    "# Pad the sequences to make sure they're all the same length\n",
    "input_data_padded = pad_sequences(input_sequences, maxlen=max_input_length)\n",
    "\n",
    "# **Pad target sequences to max_input_length or any specific length you'd like**\n",
    "output_data_padded = pad_sequences(output_sequences, maxlen=max_input_length)\n",
    "\n",
    "# One-hot encode the output data\n",
    "output_data_one_hot = to_categorical(output_data_padded, num_classes=vocab_size_output)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "# Encoder: Embedding layer for input (user messages)\n",
    "model.add(Embedding(input_dim=vocab_size_input, output_dim=128, input_length=max_input_length))\n",
    "\n",
    "# LSTM layer for encoding the input sequence\n",
    "model.add(LSTM(128, return_sequences=True))  # return_sequences=True for sequence output\n",
    "model.add(LSTM(128, return_sequences=True))  # another LSTM layer for encoding\n",
    "\n",
    "# Decoder: Dense layer for predicting the next word in the sequence (assistant response)\n",
    "model.add(Dense(vocab_size_output, activation='softmax'))  # Output for each time step\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data_padded, output_data_one_hot, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c26e0829-1a1d-4a1b-bb70-ee2e81101151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 680ms/step - accuracy: 0.0173 - loss: 4.3431 - val_accuracy: 0.1429 - val_loss: 4.3279\n",
      "Epoch 2/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0920 - loss: 4.3271 - val_accuracy: 0.1000 - val_loss: 4.3080\n",
      "Epoch 3/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1025 - loss: 4.3066 - val_accuracy: 0.1000 - val_loss: 4.2760\n",
      "Epoch 4/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.0990 - loss: 4.2728 - val_accuracy: 0.1000 - val_loss: 4.2179\n",
      "Epoch 5/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0908 - loss: 4.2121 - val_accuracy: 0.1000 - val_loss: 4.1093\n",
      "Epoch 6/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.0847 - loss: 4.1054 - val_accuracy: 0.1000 - val_loss: 3.9569\n",
      "Epoch 7/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0857 - loss: 3.9514 - val_accuracy: 0.0857 - val_loss: 3.8793\n",
      "Epoch 8/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.0769 - loss: 3.8778 - val_accuracy: 0.0857 - val_loss: 3.7953\n",
      "Epoch 9/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0830 - loss: 3.8197 - val_accuracy: 0.0857 - val_loss: 3.6563\n",
      "Epoch 10/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.0866 - loss: 3.6832 - val_accuracy: 0.0857 - val_loss: 3.5175\n",
      "Epoch 11/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.1001 - loss: 3.5278 - val_accuracy: 0.1143 - val_loss: 3.3815\n",
      "Epoch 12/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.1161 - loss: 3.3887 - val_accuracy: 0.1286 - val_loss: 3.2274\n",
      "Epoch 13/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.1164 - loss: 3.2478 - val_accuracy: 0.1571 - val_loss: 3.0562\n",
      "Epoch 14/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1265 - loss: 3.1245 - val_accuracy: 0.1571 - val_loss: 2.8980\n",
      "Epoch 15/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1454 - loss: 2.9582 - val_accuracy: 0.1571 - val_loss: 2.7450\n",
      "Epoch 16/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.1296 - loss: 2.8372 - val_accuracy: 0.2000 - val_loss: 2.5952\n",
      "Epoch 17/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1720 - loss: 2.7092 - val_accuracy: 0.2429 - val_loss: 2.4572\n",
      "Epoch 18/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.1887 - loss: 2.5636 - val_accuracy: 0.3143 - val_loss: 2.3294\n",
      "Epoch 19/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2179 - loss: 2.4478 - val_accuracy: 0.3000 - val_loss: 2.2134\n",
      "Epoch 20/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2347 - loss: 2.3207 - val_accuracy: 0.3714 - val_loss: 2.1040\n",
      "Epoch 21/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2408 - loss: 2.2516 - val_accuracy: 0.3857 - val_loss: 2.0115\n",
      "Epoch 22/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2740 - loss: 2.1909 - val_accuracy: 0.3857 - val_loss: 1.9296\n",
      "Epoch 23/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3098 - loss: 2.0709 - val_accuracy: 0.3429 - val_loss: 1.8483\n",
      "Epoch 24/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3256 - loss: 2.0105 - val_accuracy: 0.4571 - val_loss: 1.7623\n",
      "Epoch 25/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3323 - loss: 1.9408 - val_accuracy: 0.5714 - val_loss: 1.6956\n",
      "Epoch 26/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3643 - loss: 1.8598 - val_accuracy: 0.5714 - val_loss: 1.6405\n",
      "Epoch 27/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3506 - loss: 1.8067 - val_accuracy: 0.6143 - val_loss: 1.5765\n",
      "Epoch 28/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3810 - loss: 1.7694 - val_accuracy: 0.6143 - val_loss: 1.5209\n",
      "Epoch 29/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4220 - loss: 1.7009 - val_accuracy: 0.6143 - val_loss: 1.4779\n",
      "Epoch 30/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.3762 - loss: 1.6626 - val_accuracy: 0.6857 - val_loss: 1.4321\n",
      "Epoch 31/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.3988 - loss: 1.6295 - val_accuracy: 0.7143 - val_loss: 1.3899\n",
      "Epoch 32/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4335 - loss: 1.5362 - val_accuracy: 0.6857 - val_loss: 1.3529\n",
      "Epoch 33/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4478 - loss: 1.5100 - val_accuracy: 0.7429 - val_loss: 1.3183\n",
      "Epoch 34/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4662 - loss: 1.5033 - val_accuracy: 0.7714 - val_loss: 1.2822\n",
      "Epoch 35/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.4680 - loss: 1.4545 - val_accuracy: 0.7714 - val_loss: 1.2572\n",
      "Epoch 36/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4551 - loss: 1.4489 - val_accuracy: 0.7714 - val_loss: 1.2275\n",
      "Epoch 37/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4789 - loss: 1.4329 - val_accuracy: 0.7571 - val_loss: 1.1984\n",
      "Epoch 38/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4753 - loss: 1.3993 - val_accuracy: 0.7714 - val_loss: 1.1746\n",
      "Epoch 39/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5189 - loss: 1.3312 - val_accuracy: 0.8286 - val_loss: 1.1508\n",
      "Epoch 40/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.5504 - loss: 1.3229 - val_accuracy: 0.8857 - val_loss: 1.1191\n",
      "Epoch 41/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5451 - loss: 1.2653 - val_accuracy: 0.8857 - val_loss: 1.0967\n",
      "Epoch 42/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.5204 - loss: 1.2958 - val_accuracy: 0.8714 - val_loss: 1.0714\n",
      "Epoch 43/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5565 - loss: 1.2652 - val_accuracy: 0.9286 - val_loss: 1.0455\n",
      "Epoch 44/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.5713 - loss: 1.2107 - val_accuracy: 0.9143 - val_loss: 1.0235\n",
      "Epoch 45/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.5427 - loss: 1.2052 - val_accuracy: 0.8714 - val_loss: 1.0240\n",
      "Epoch 46/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5292 - loss: 1.2472 - val_accuracy: 0.8571 - val_loss: 1.0323\n",
      "Epoch 47/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.5635 - loss: 1.2159 - val_accuracy: 0.9286 - val_loss: 0.9793\n",
      "Epoch 48/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.5510 - loss: 1.1812 - val_accuracy: 0.9286 - val_loss: 0.9628\n",
      "Epoch 49/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6137 - loss: 1.1286 - val_accuracy: 0.9143 - val_loss: 0.9612\n",
      "Epoch 50/50\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5990 - loss: 1.1415 - val_accuracy: 0.8714 - val_loss: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ebc6ee8a50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Model Architecture Improvements\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding Layer\n",
    "model.add(Embedding(input_dim=vocab_size_input, output_dim=128, input_length=max_input_length))\n",
    "\n",
    "# Bidirectional LSTM Layer\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True)))  # Bidirectional LSTM\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "\n",
    "# LSTM Layer\n",
    "model.add(LSTM(128, return_sequences=True))  # Another LSTM layer\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(vocab_size_output, activation='softmax'))  # Output layer for each time step\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "optimizer = Adam(learning_rate=0.001)  # You can try tuning the learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_data_padded, output_data_one_hot, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7143e3ce-cf16-4786-ad8c-5e04939df317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Predicted Response:\n",
      " - test\n",
      "- drive\n",
      "- access\n",
      "- from\n",
      "- another\n",
      "- user\n",
      "- account\n",
      "- escalate\n",
      "- to\n",
      "- reset\n",
      "- controls\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the model (if you have saved it, otherwise continue using your trained model)\n",
    "# model = load_model('your_model_path')\n",
    "\n",
    "# Set the temperature and other parameters\n",
    "temperature = 0.7  # Adjust the temperature for more control over output randomness\n",
    "\n",
    "def predict_response(input_text, model, max_input_length, input_tokenizer, output_tokenizer):\n",
    "    # Convert input text to sequences\n",
    "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
    "    input_data_padded = pad_sequences(input_sequence, maxlen=max_input_length)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(input_data_padded, verbose=0)\n",
    "    \n",
    "    # Apply temperature to the predictions to reduce repetition\n",
    "    predictions = predictions / temperature\n",
    "    predictions = np.exp(predictions)  # Apply softmax temperature scaling\n",
    "    predictions = predictions / np.sum(predictions, axis=-1, keepdims=True)  # Normalize again\n",
    "\n",
    "    # Get the word with the highest probability (Argmax for prediction)\n",
    "    predicted_word_indices = predictions.argmax(axis=-1)\n",
    "\n",
    "    # Convert indices to words using output_tokenizer\n",
    "    predicted_words = output_tokenizer.sequences_to_texts(predicted_word_indices)\n",
    "    \n",
    "    # Post-process: Format into bullet points\n",
    "    # Assuming each line contains separate troubleshooting steps\n",
    "    formatted_output = format_response(predicted_words[0])\n",
    "\n",
    "    return formatted_output\n",
    "\n",
    "def format_response(response_text):\n",
    "    # Post-process to format output as bullet points, removing unnecessary words\n",
    "    formatted_response = response_text.strip().split()  # Split words into a list\n",
    "\n",
    "    # Remove redundancy, if any (optional: this could be more complex for better handling)\n",
    "    # Convert list to set and back to list to remove duplicates, preserving word order.\n",
    "    unique_words = list(dict.fromkeys(formatted_response))  # Removes duplicates while maintaining order\n",
    "\n",
    "    # Join into a single string with bullet points\n",
    "    return \"\\n- \".join(unique_words)\n",
    "\n",
    "# Example of usage:\n",
    "test_data = \"The shared network drive is inaccessible; it's saying permission denied.\"\n",
    "formatted_response = predict_response(test_data, model, max_input_length, input_tokenizer, output_tokenizer)\n",
    "\n",
    "print(\"Formatted Predicted Response:\\n\", \"- \" + formatted_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c6f2237-8b39-45fc-babf-ebf471750115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge-score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: absl-py in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\manasi\\anaconda3\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24971 sha256=9b92ede810921effcd7bffccb2d7bf0d70d5ef29a903f3d99a81624f1a839d03\n",
      "  Stored in directory: c:\\users\\manasi\\appdata\\local\\pip\\cache\\wheels\\1e\\19\\43\\8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a587c0ba-798d-45ba-8a85-9ebd8af14d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 F1: 0.28571428571428575\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Assuming you have the validation set and the model's responses\n",
    "reference_summaries = [\n",
    "    \"Expected assistant summaries for validation data.\"\n",
    "]\n",
    "predicted_summaries = [\n",
    "    \"Generated assistant summaries from the fine-tuned model.\"\n",
    "]\n",
    "\n",
    "# Initialize the ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
    "\n",
    "# Calculate ROUGE-1 F1\n",
    "scores = [scorer.score(ref, pred) for ref, pred in zip(reference_summaries, predicted_summaries)]\n",
    "rouge_1_f1 = [score['rouge1'].fmeasure for score in scores]\n",
    "\n",
    "print(f\"ROUGE-1 F1: {sum(rouge_1_f1) / len(rouge_1_f1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42644d4b-65bb-415d-b986-7be6d990f73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User Query: I can't connect to the company VPN from home, what should I do?\n",
      "Assistant Response (Before): Please check your network settings and try again.\n",
      "\n",
      "Proposed Prompt Template Tweak (To guide assistant to provide complete action items):\n",
      "\n",
      "    User query: \"I can't connect to the company VPN from home, what should I do?\"\n",
      "    Assistant response: \"Provide a step-by-step troubleshooting guide with clear actions for the user, including things like checking network settings, verifying VPN credentials, restarting the VPN service, and escalation to IT if necessary.\"\n",
      "    \n",
      "\n",
      "Assistant Response (After Post-Processing):\n",
      "Please check your network settings and try again.\n",
      "\n",
      "Action: Please ensure you check network.\n",
      "\n",
      "Action: Please ensure you restart.\n",
      "\n",
      "Action: Please ensure you contact support.\n",
      "\n",
      "Action: Please ensure you verify credentials.\n",
      "\n",
      "User Query: What is the status of my IT support ticket?\n",
      "Assistant Response (Before): Your support ticket is currently being reviewed by our IT team. They are analyzing the issue to identify the cause, and once they have completed their investigation, they will inform you of the next steps. The expected timeline for resolution is approximately 24 hours, depending on the complexity of the issue. In the meantime, please feel free to check back or contact our team if you have any further questions.\n",
      "\n",
      "Proposed Prompt Template Tweak (To guide assistant to be more concise):\n",
      "\n",
      "    User query: \"What is the status of my IT support ticket?\"\n",
      "    Assistant response: \"Provide a concise update on the ticket status. Avoid unnecessary details and focus on the most relevant information (e.g., current status and estimated resolution time).\"\n",
      "    \n",
      "\n",
      "Assistant Response (After Post-Processing - Shortened):\n",
      "Your support ticket is currently being reviewed by our IT team. They are analyzing the issue to iden...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "\n",
    "# Sample data for analysis (emulating assistant outputs)\n",
    "failure_case_1 = {\n",
    "    \"user_query\": \"I can't connect to the company VPN from home, what should I do?\",\n",
    "    \"assistant_response\": \"Please check your network settings and try again.\"\n",
    "}\n",
    "\n",
    "failure_case_2 = {\n",
    "    \"user_query\": \"What is the status of my IT support ticket?\",\n",
    "    \"assistant_response\": \"Your support ticket is currently being reviewed by our IT team. They are analyzing the issue to identify the cause, and once they have completed their investigation, they will inform you of the next steps. The expected timeline for resolution is approximately 24 hours, depending on the complexity of the issue. In the meantime, please feel free to check back or contact our team if you have any further questions.\"\n",
    "}\n",
    "\n",
    "# --- Failure Case 1: Missing Action Items ---\n",
    "# We need to ensure that the assistant provides a full troubleshooting guide.\n",
    "def analyze_missing_action_items(failure_case):\n",
    "    print(f\"\\nUser Query: {failure_case['user_query']}\")\n",
    "    print(f\"Assistant Response (Before): {failure_case['assistant_response']}\")\n",
    "    \n",
    "    # --- Prompt Template Tweak ---\n",
    "    # Adding more explicit instructions for the model to include a complete troubleshooting guide.\n",
    "    prompt_tweak = \"\"\"\n",
    "    User query: \"I can't connect to the company VPN from home, what should I do?\"\n",
    "    Assistant response: \"Provide a step-by-step troubleshooting guide with clear actions for the user, including things like checking network settings, verifying VPN credentials, restarting the VPN service, and escalation to IT if necessary.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nProposed Prompt Template Tweak (To guide assistant to provide complete action items):\")\n",
    "    print(prompt_tweak)\n",
    "\n",
    "    # --- Post-Processing Rule ---\n",
    "    # Implementing a post-processing rule to enforce the inclusion of key action items.\n",
    "    required_actions = [\"check network\", \"restart\", \"contact support\", \"verify credentials\"]\n",
    "\n",
    "    def enforce_action_items(response):\n",
    "        for action in required_actions:\n",
    "            if action not in response.lower():\n",
    "                response += f\"\\n\\nAction: Please ensure you {action}.\"\n",
    "        return response\n",
    "\n",
    "    # Post-processing on assistant response\n",
    "    processed_response = enforce_action_items(failure_case['assistant_response'])\n",
    "    print(\"\\nAssistant Response (After Post-Processing):\")\n",
    "    print(processed_response)\n",
    "\n",
    "\n",
    "# --- Failure Case 2: Verbosity ---\n",
    "# We need to make the response more concise.\n",
    "def analyze_verbosity(failure_case):\n",
    "    print(f\"\\nUser Query: {failure_case['user_query']}\")\n",
    "    print(f\"Assistant Response (Before): {failure_case['assistant_response']}\")\n",
    "    \n",
    "    # --- Prompt Template Tweak ---\n",
    "    # Provide a more concise prompt to the assistant.\n",
    "    prompt_tweak = \"\"\"\n",
    "    User query: \"What is the status of my IT support ticket?\"\n",
    "    Assistant response: \"Provide a concise update on the ticket status. Avoid unnecessary details and focus on the most relevant information (e.g., current status and estimated resolution time).\"\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\nProposed Prompt Template Tweak (To guide assistant to be more concise):\")\n",
    "    print(prompt_tweak)\n",
    "\n",
    "    # --- Post-Processing Rule ---\n",
    "    # Implementing a post-processing rule to limit verbosity.\n",
    "    def shorten_response(response, max_length=100):\n",
    "        if len(response) > max_length:\n",
    "            response = response[:max_length] + \"...\"\n",
    "        return response\n",
    "\n",
    "    # Post-processing on assistant response\n",
    "    shortened_response = shorten_response(failure_case['assistant_response'])\n",
    "    print(\"\\nAssistant Response (After Post-Processing - Shortened):\")\n",
    "    print(shortened_response)\n",
    "\n",
    "\n",
    "# --- Execute the Analysis ---\n",
    "# Analyzing failure case 1 (missing action items)\n",
    "analyze_missing_action_items(failure_case_1)\n",
    "\n",
    "# Analyzing failure case 2 (verbosity)\n",
    "analyze_verbosity(failure_case_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08549c02-0a58-4bcc-a849-0fb3b2bf1bd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c72c9c-cd5c-490d-8906-84cdf4f41438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
